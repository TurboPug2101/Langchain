{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() ## aloading all the environment variable\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001751E97E310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001751EAC7010>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"Gemma2-9b-It\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's awesome, Ishank!  \\n\\nWhat kind of projects do you like to build? \\n\\nDo you like working with your hands, using tools, or are you more into coding or designing? \\n\\nTell me more about your favorite projects!  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 20, 'total_tokens': 77, 'completion_time': 0.103636364, 'prompt_time': 0.00021314, 'queue_time': 0.025576619, 'total_time': 0.103849504}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-c08d0563-68da-44e2-8ba1-a6d252e69d5d-0', usage_metadata={'input_tokens': 20, 'output_tokens': 57, 'total_tokens': 77})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "# humanmsg indicates user is giving msg\n",
    "\n",
    "model.invoke([HumanMessage(content=\"I am ishank, i like playing and building projects\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Ishank, and you like to play and build projects! \\n\\nIs there anything specific you'd like to talk about regarding your projects or hobbies? I'm happy to chat more! üòä  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 94, 'total_tokens': 140, 'completion_time': 0.083636364, 'prompt_time': 0.002496342, 'queue_time': 0.011286998, 'total_time': 0.086132706}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e9ff819d-d623-480e-ac64-17f56bbd2f49-0', usage_metadata={'input_tokens': 94, 'output_tokens': 46, 'total_tokens': 140})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"I am ishank, i like playing and building projects\"),\n",
    "        AIMessage(content=\"That's awesome, Ishank!  \\n\\nWhat kind of projects do you like to build? \\n\\nDo you like working with your hands, using tools, or are you more into coding or designing? \\n\\nTell me more about your favorite projects!\"),\n",
    "        HumanMessage(content=\"what is my name and what do i like to do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# runnables for chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ChatMessageHistory : Manages and stores chat history within LangChain for tracking conversation state across interactions.\n",
    "2. RunnableWithMessageHistory : Executes a runnable (e.g., a chain or function) while keeping track of the associated chat message history for further processing or evaluation.\n",
    "3. BaseChatMessageHistory : if you wanted to store chat history in a database, you could create a class that inherits from BaseChatMessageHistory and implements the required methods for saving and retrieving chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}\n",
    "# fn to retrieve chagt history based on session id\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "# ses id used to distinuish sessions\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]   \n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the session ID is not in the store:\n",
    "\n",
    "It creates a new ChatMessageHistory object for that session.\n",
    "This new history is stored in the store dictionary under the key session_id.\n",
    "Returns the session's chat history:\n",
    "\n",
    "If the session already exists, it returns the previously stored history.\n",
    "If it's a new session, it returns the newly created empty chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "# These key names are part of the internal structure that LangChain uses to process configurations and manage sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's great to meet you, Ishank!  \\n\\nI'm curious, what kind of projects do you like to build? üõ†Ô∏è\\n\\nDo you enjoy working with wood, electronics, coding, or something totally different? \\n\\nTell me about a project you're really proud of!  üòä \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 92, 'total_tokens': 161, 'completion_time': 0.125454545, 'prompt_time': 0.006746988, 'queue_time': 0.006379762, 'total_time': 0.132201533}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-6f20f320-958f-4bd8-a4fb-fc84a29d8c20-0', usage_metadata={'input_tokens': 92, 'output_tokens': 69, 'total_tokens': 161})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content='I am ishank, i like playing and building projects')],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Ishank! üòä  \\n\\nIs there anything else you'd like to talk about, Ishank?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 256, 'total_tokens': 283, 'completion_time': 0.049090909, 'prompt_time': 0.037690938, 'queue_time': 0.050889291999999996, 'total_time': 0.086781847}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-7f49ce5b-5f77-4095-9e31-966470df1c15-0', usage_metadata={'input_tokens': 256, 'output_tokens': 27, 'total_tokens': 283})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content='what is my name?')],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Ishank.  \\n\\nDid you have something else in mind? üòä \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the config name, chat is kept same\n",
    "config22={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content='what is my name?')],\n",
    "    config=config22\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'd be happy to know! üòä  \\n\\n\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the config session id\n",
    "# chat2\n",
    "config22={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content='what is my name?')],\n",
    "    config=config22\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John, it's nice to meet you! üëã \\n\\nIs there anything I can help you with today?\\n\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content='My name is John')],\n",
    "    config=config22\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is Ishank.  \\n\\nI'm happy to help remind you! üòä  \\n\\nIs there something else you'd like to talk about?  \\n\\n\\n\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content='what is my name ?')],\n",
    "    config=config\n",
    "    # config=config22\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When you change the session_id: You are either:\n",
    "Continuing a chat session by referencing its past messages.\n",
    "<!--  -->\n",
    "Starting a new chat session by providing a new session_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Amnswer all the question to the nest of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "        # MessagesPlaceholder to dynamically insert the chat history\n",
    "    ]\n",
    ")\n",
    "# key val pairs \n",
    "chain =prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Mike! It's nice to meet you.  \\n\\nHow can I help you today? üòÑ \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 31, 'total_tokens': 56, 'completion_time': 0.045454545, 'prompt_time': 0.000341108, 'queue_time': 0.012812282, 'total_time': 0.045795653}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-335d887f-5927-46c7-8cda-cfef14ff45eb-0', usage_metadata={'input_tokens': 31, 'output_tokens': 25, 'total_tokens': 56})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is Mike\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)\n",
    "# chain is prompt and model\n",
    "\n",
    "# RunnableWithMessageHistory: This is a class from LangChain that adds message history functionality to a runnable object (in this case, chain).\n",
    "\n",
    "# chain: This is the actual runnable chain that processes inputs (messages) and generates responses.\n",
    "# get_session_history: This function is used to retrieve the session history for a specific session_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Mike! It's nice to meet you.\\n\\nWhat can I do for you today?  üòä  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 31, 'total_tokens': 57, 'completion_time': 0.047272727, 'prompt_time': 0.000336829, 'queue_time': 0.012969172, 'total_time': 0.047609556}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-47daa44e-97bf-497b-81bd-b8cb2b40b86b-0', usage_metadata={'input_tokens': 31, 'output_tokens': 26, 'total_tokens': 57})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Mike\")],\n",
    "    config=config,\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§∏‡•Å‡§™‡§∞‡§Æ‡•à‡§®! ü¶∏\\u200d‚ôÇÔ∏è\\n\\n‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§™‡§®‡•Ä ‡§™‡•Ç‡§∞‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•Ç‡§Å‡§ó‡§æ ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á ‡§∏‡§ï‡•Ç‡§Å‡•§ üòä \\n\\n‡§Ü‡§™ ‡§ï‡•ç‡§Ø‡§æ ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç? \\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"I am superman\")],\"language\":\"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now wrapping this chain in message history class\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§π‡§æ‡§Å, ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ Batman ‡§π‡•à!  üòä  \\n\\n‡§Ü‡§™‡§ï‡•ã ‡§ï‡•Å‡§õ ‡§î‡§∞ ‡§ö‡§æ‡§π‡§ø‡§è?  \\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"My name is batman\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ Mike ‡§π‡•à‡•§  \\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What is my name?\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ Batman ‡§π‡•à‡•§  \\n\\n‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§ï‡•ã‡§à ‡§î‡§∞ ‡§∏‡§µ‡§æ‡§≤ ‡§™‡•Ç‡§õ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç? \\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"What is my name?\")],\"language\":\"Hindi\"},\n",
    "    config=config\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "<!--  -->\n",
    "'trim_messages' helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18905fefc4734f73a459122aaae58ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAi\\Langchain\\langvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ishan\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20027308f83a4cafaa399822e1207029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1f9b4e3aeb4c2494d6ed4074f25cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ed91119ada48439f36df22ece9e166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba36902e324e425bac19c1f8741ae7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GenAi\\Langchain\\langvenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "# trim- .. helper helps reduce how many messages we are sending to model\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "# focus on last convo it had , count from there\n",
    "token_counter=model,\n",
    "include_system=True,\n",
    "allow_partial=False,\n",
    "start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "trimmer.invoke(messages)\n",
    "\n",
    "# top 2 messages trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(itemgetter('messages'))\n",
       "| RunnableLambda(...)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemgetter(\"messages\")|trimmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a Chain with trimmer function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"What is 2 + 2?\"  üòä \\n\\n\\nLet me know if you want to try another one!\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "# n LangChain, RunnablePassthrough is a type of Runnable that essentially passes its input directly to its output without modifying or transforming it\n",
    "\n",
    "chain=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    |prompt\n",
    "    | model\n",
    "\n",
    ")\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What math problem did i ask?\" )],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history= RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
